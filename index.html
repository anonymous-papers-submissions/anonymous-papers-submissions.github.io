<!doctype html>
<html lang="en-GB" class="no-js fixed-nav">
<head>
	<meta charset="UTF-8">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="format-detection" content="telephone=no">
	<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />

	<title>UniSim: Learning Interactive Real-World Simulators</title>


	<link rel='stylesheet' id='wayve-all-css' href='style.css' type='text/css' media='all' />
	<link rel='stylesheet' id='wayve-all-css' href='custom.css' type='text/css' media='all' />
	<script src="scripts.js" defer></script>
	    
</head>

		

<article class='blog-post'>

<nav>
    <ul>
        <li><a href="#action-rich" data-scroll>Action-Rich</a></li>
        <li><a href="#long-horizon" data-scroll>Long-Horizon</a></li>
        <li><a href="#diversity" data-scroll>Diversity</a></li>
        <li><a href="#planning" data-scroll>Planning</a></li>
        <li><a href="#rl" data-scroll>RL</a></li>
    </ul>
</nav>
  

	<div class=blog-intro-container>
		<h1 class='too-long'>UniSim: Learning Interactive Real-World Simulators</h1>


		<div class=intro>
			<p><span style="font-weight: 400;">Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions carried out by humans, robots, and other types of interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies to training embodied agents purely in simulation that can be directly deployed in the real world. In this work, we explore these possibilities around learning a universal simulator (UniSim) of real-world interactions through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., rich labeled objects in image data, rich actions in robotics data, and rich movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move to x, y location” from otherwise static scenes and objects. Use cases for such a real-world simulator are vast. As an example, we use UniSim to simulate interactive experiences to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit significant real-world transfer from purely training in a real-world like simulator. Lastly, we show that other types of intelligence such as video captioning and detection models can also benefit from simulated experiences in UniSim, opening up even wider applications of a real-world simulator.</p>
<p><a href="materials/paper.pdf" target="_blank" rel="noopener" data-display-type="button" class="custom-button">paper</a></p>
		</div>
		<div class=blog-meta>
			<div>
							</div>
			<div>
				<span class=reading-time></span>
			</div>
		</div>
	</div>

    <video class=banner loop muted playsinline autoplay height=580><source src="materials/banner.mp4"></video>


<div class="copy" >      
<div class="copy" >  
  <div class=content id="action-rich">
    <h2><span style="font-weight: 400;">Action-Rich Simulations</span></h2>
<p><span style="font-weight: 400;">One major difference between an interactive real-world simulator and typical video generation is that a simulator should support a diverse set of actions. Below, we demonstrate how UniSim can simulate different experiences given different actions starting from the same initial frame.</span></p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/kitchen_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/switch_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/chapel_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/golden_gate_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="long-horizon">    
    <h2><span style="font-weight: 400;">Long-Horizon Simulations</span></h2>
    <p>
      The true value of UniSim lies in simulating long episodes to enable optimizing decisions through search, planning, optimal control, or reinforcement learning. Below, we demonstrate how UniSim can simulate interactive experiences with long interaction horizons.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/fractal_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/play_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="diversity">    
    <h2><span style="font-weight: 400;">Diversity and Stochasticity</span></h2>
    <p>
      UniSim can also support highly diverse and stochastic environment transitions, such as diversity in objects being revealed after removing the towel, as well as locations and colors of objects being placed, as illustrated below.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/uncover_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/put_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="planning">    
    <h2><span style="font-weight: 400;">Long-Horizon Planning with UniSim</span></h2>
    <p>
      UniSim can be used to train other machine intelligence such as embodied planners. We concatenate long-horizon instructions and generate videos by doing repeated rollouts in UniSim. The resulting video and instruction data can then be used to train image-goal conditioned vision-language model (VLM) policies. The simulated plans and real-robot executions are shown below.      
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/hindsight_word.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content id="rl">    
    <h2><span style="font-weight: 400;">Reinforcement Learning with UniSim</span></h2>
    <p> UniSim can enable effective training of RL agents by providing the agent with a realistic simulator that can be accessed in parallel. Below, we demonstrate the simulated rollouts of a RL policy trained using policy gradient.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_sim_video-slow.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content>
    <p> We then deploy the RL policy trained in UniSim onto the real robot. The executions are shown below. We additionally report the number of steps until task completion predicted by a reward model (this is also the reward model we use to train the RL agent). The RL policy trained purely in simulation generalizes to the real robot in zero-shot.
    </p>
  </div>
</div>


<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_real_video-fast.mp4"></video>
  </span>
</div>


</html>


